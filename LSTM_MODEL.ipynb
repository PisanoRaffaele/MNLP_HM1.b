{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOHJpfleHdYzC0ZG9fjhdOW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PisanoRaffaele/MNLP_HM1.b/blob/main/LSTM_MODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#START"
      ],
      "metadata": {
        "id": "E_SEo0v0pqo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import Vocab, vocab, Vectors\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random\n",
        "\n",
        "import os\n",
        "\n",
        "!pip install fasttext\n",
        "import fasttext\n",
        "import fasttext.util"
      ],
      "metadata": {
        "id": "f74XiLMIqBQf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "143f5fc3-4cb3-4722-956c-7785bc8c7ec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/68.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m61.4/68.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.12.0-py3-none-any.whl (234 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.25.2)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4227144 sha256=6de68d301fddc9967c845add3cb81b5f5d320fd4505303a1b556aaae22fae78e\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ],
      "metadata": {
        "id": "ufoz8paSoA0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/PisanoRaffaele/MNLP_HM1.b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8FVMt2bpqGX",
        "outputId": "931956b8-f540-4097-fed2-8ed7a7143a57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MNLP_HM1.b'...\n",
            "remote: Enumerating objects: 8, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 8 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (8/8), 571.23 KiB | 2.36 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_news_path = Path(\"MNLP_HM1.b/test-news-taskA.jsonl\")\n",
        "test_tweets_path = Path(\"MNLP_HM1.b/test-tweets-taskA.jsonl\")\n",
        "development_path = Path(\"MNLP_HM1.b/train-taskA.jsonl\")\n",
        "validation_path = Path(\"MNLP_HM1.b/validation.jsonl\")\n",
        "train_path = Path(\"MNLP_HM1.b/train.jsonl\")\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/MNLP_HM1.b/cc.it.300.bin\""
      ],
      "metadata": {
        "id": "SOBMe-dZpuJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creation of validation set"
      ],
      "metadata": {
        "id": "2beQnv_Gn9HD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 1\n",
        "random.seed(seed)\n",
        "\n",
        "with open(development_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    dev_data = [json.loads(line.strip()) for line in file]\n",
        "\n",
        "shuffled_data = random.sample(dev_data, len(dev_data))\n",
        "len_val = int(len(shuffled_data) * 0.2)\n",
        "\n",
        "with open(validation_path, \"w\", encoding=\"utf-8\") as file:\n",
        "    for item in shuffled_data[:len_val]:\n",
        "        file.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "with open(train_path, \"w\", encoding=\"utf-8\") as file:\n",
        "    for item in shuffled_data[len_val:]:\n",
        "        file.write(json.dumps(item) + \"\\n\")"
      ],
      "metadata": {
        "id": "5JScdYaO01YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_hate_text(dataset: str, title: str):\n",
        "    counter = Counter()\n",
        "\n",
        "    with open(dataset) as f:\n",
        "        for line in f:\n",
        "            sample = json.loads(line.strip())\n",
        "            if (sample['label'] == 0):\n",
        "              counter[0] += 1\n",
        "            else:\n",
        "              counter[1] += 1\n",
        "\n",
        "    l = sorted(counter)\n",
        "    values = [counter[i] for i in l]\n",
        "\n",
        "    plt.bar(l, values)\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "lewgbSPSqH99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_hate_text(train_path, 'training')"
      ],
      "metadata": {
        "id": "MhzGOH0FnNgZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "b30f67fc-6fdd-455b-e335-2ed070ae402f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_hate_text(validation_path, 'validation')"
      ],
      "metadata": {
        "id": "BcSfKsFvndPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_hate_text(test_news_path, 'news')"
      ],
      "metadata": {
        "id": "UuTrZKo1nhHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_hate_text(test_tweets_path, 'tweets')"
      ],
      "metadata": {
        "id": "LoYI8NkRpF_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to save results on drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "model = fasttext.load_model(model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WWzRRZ4kWWB",
        "outputId": "f96d4a8b-fcb6-482c-84c5-4104e01ed281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_words_list = model.get_words()"
      ],
      "metadata": {
        "id": "vLXNwW__kdIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_words = {}\n",
        "for i, elem in enumerate(embedded_words_list):\n",
        "    embedded_words[elem] = i"
      ],
      "metadata": {
        "id": "GvGcecy0flw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset"
      ],
      "metadata": {
        "id": "fAd1fBJGPIuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I've noticed that, after the tokenization with the spacy italian tokenizer, a lot of the strings that haven't a corresponding word embedding in the fasttext model are strings not well tokenized:\n",
        "- they starts or end with ' ( '; ' ) ' char;\n",
        "- they have symbols between two or more words like:\n",
        "  - \"   \n",
        "  - _\n",
        "  - !\n",
        "  - -\n",
        "  - ?\n",
        "  - ’\n",
        "  - '\n",
        "\n",
        "- There were a lot of words separated by a dot like:\n",
        " - cosa.che\n",
        " - .intolleranti\n",
        " - zingari.non\n",
        "\n",
        "\n",
        "And there were a some 'common' words not finded in the embeddings that\n",
        "\n",
        "I've have decided to modify the tokenizer s.t after the first tokenization i deal with the problems above with the custom tokenizer."
      ],
      "metadata": {
        "id": "qdwOzMBAQHiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#all this stuff is applied after the tokenization by the spacy tokenizer and it is a lot based on the output of the words do not tokenized.\n",
        "\n",
        "def splitchar(token, char):\n",
        "    ret = []\n",
        "    splitted = token.split(char)\n",
        "    i = 0\n",
        "    for sub_token in splitted:\n",
        "        if sub_token:\n",
        "            ret.extend(checkToken(sub_token))\n",
        "        if i != len(splitted) - 1:\n",
        "            ret.append(char)\n",
        "        i += 1\n",
        "    return ret\n",
        "\n",
        "def checkdots(token):\n",
        "    pos_first = token.find('.')\n",
        "    if token == '.':\n",
        "        return [token]\n",
        "    if len(token) > pos_first + 1 != None and token[pos_first + 1] == '.':\n",
        "        return [token]\n",
        "    if pos_first > 0 and len(token) > pos_first + 1:\n",
        "        if token[pos_first - 1].isdigit() and token[pos_first + 1].isdigit():\n",
        "            return [token]\n",
        "    if len(token) > pos_first + 2 and token[pos_first + 2] == '.': #case like S.O.S. and all acronyms -> do not want to split them\n",
        "        return [token]\n",
        "\n",
        "    ret = []\n",
        "    splitted = token.split('.', 1)\n",
        "    i = 0\n",
        "    for sub_token in splitted:\n",
        "        if sub_token:\n",
        "            ret.extend(checkToken(sub_token))\n",
        "        if i != len(splitted) - 1:\n",
        "            ret.append('.')\n",
        "        i += 1\n",
        "    return ret\n",
        "\n",
        "def checkToken(token):\n",
        "  new_tokens = []\n",
        "  if '\"' in token:\n",
        "      new_tokens.extend(splitchar(token, '\"'))\n",
        "  elif ')' in token:\n",
        "      new_tokens.extend(splitchar(token, ')'))\n",
        "  elif '(' in token:\n",
        "      new_tokens.extend(splitchar(token, '('))\n",
        "  elif '-' in token:\n",
        "      new_tokens.extend(splitchar(token, '-'))\n",
        "  elif '_' in token:\n",
        "      new_tokens.extend(splitchar(token, '_'))\n",
        "  elif '!' in token:\n",
        "      new_tokens.extend(splitchar(token, '!'))\n",
        "  elif '?' in token:\n",
        "      new_tokens.extend(splitchar(token, '?'))\n",
        "  elif \"'\" in token:\n",
        "      new_tokens.extend(splitchar(token, \"'\"))\n",
        "  elif '’' in token:\n",
        "      new_tokens.extend(splitchar(token, '’'))\n",
        "  elif '”' in token:\n",
        "      new_tokens.extend(splitchar(token, '”'))\n",
        "  elif '\\\\' in token:\n",
        "      new_tokens.extend(splitchar(token, '\\\\'))\n",
        "  elif '/' in token:\n",
        "      new_tokens.extend(splitchar(token, '/'))\n",
        "  elif '•' in token:\n",
        "      new_tokens.extend(splitchar(token, '•'))\n",
        "  elif ',' in token:\n",
        "      new_tokens.extend(splitchar(token, ','))\n",
        "  elif '#' in token:\n",
        "      new_tokens.extend(splitchar(token, '#'))\n",
        "  elif '.' in token:\n",
        "      new_tokens.extend(checkdots(token))\n",
        "  else:\n",
        "      new_tokens.append(token)\n",
        "  return new_tokens\n",
        "\n",
        "\n",
        "def custom_tokenizer(text, tokenizer):\n",
        "    tokens = tokenizer(text)\n",
        "    final_tokens = []\n",
        "\n",
        "    for token in tokens:\n",
        "        if token in embedded_words or token.lower() in embedded_words:\n",
        "            final_tokens.append(token)\n",
        "        else:\n",
        "          final_tokens.extend(checkToken(token))\n",
        "    return final_tokens"
      ],
      "metadata": {
        "id": "ZoPVjN-yMlkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tests\n",
        "\n",
        "!python -m spacy download it_core_news_sm\n",
        "# tokenizer = get_tokenizer(\"spacy\", language='it')\n",
        "# print(custom_tokenizer(\"(few)))wef)\\\")) )ervre) ))\", tokenizer))\n",
        "# print(custom_tokenizer(\"?!No\", tokenizer))"
      ],
      "metadata": {
        "id": "x2PmAjO7spWz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8c2cb11-9bcd-4fad-b005-1ca0a744f49f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting it-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.7.0/it_core_news_sm-3.7.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from it-core-news-sm==3.7.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (2.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->it-core-news-sm==3.7.0) (2.1.5)\n",
            "Installing collected packages: it-core-news-sm\n",
            "Successfully installed it-core-news-sm-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SST2Dataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_file: Path,\n",
        "        max_length: int = 128,\n",
        "        device: str = \"cuda\"\n",
        "    ):\n",
        "        tokenizer = get_tokenizer(\"spacy\", language='it_core_news_sm')\n",
        "\n",
        "        self.samples = []\n",
        "        with open(input_file, \"r\") as f:\n",
        "            for line in f:\n",
        "                sample = json.loads(line.strip())\n",
        "                sample[\"tokens\"] = custom_tokenizer(sample[\"text\"], tokenizer)\n",
        "                self.samples.append(sample)\n",
        "\n",
        "        self.indexed_data: list[dict] | None = None\n",
        "\n",
        "        # Keep track of the maximum length to allow for a batch\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Save device\n",
        "        self.device = torch.device(device)\n",
        "\n",
        "        # The variable padding_id is used to keep track of the numeric ID used for the padding token within the vocabulary\n",
        "        self.padding_id: int | None = None\n",
        "\n",
        "    #returns the dict cooresponding to the sentence in position idx\n",
        "    def get_raw_element(self, idx: int) -> dict:\n",
        "        return self.samples[idx]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> dict:\n",
        "        if self.indexed_data is None:\n",
        "            raise RuntimeError(\n",
        "                \"Trying to retrieve samples but dataset has not been indexed yet!\"\n",
        "                + \" Be sure to call `.index()` on this object.\"\n",
        "                + \" If you want to retrieve raw elements, call `.get_raw_elements(idx)\"\n",
        "            )\n",
        "        return self.indexed_data[idx]\n",
        "\n",
        "    def get_vocabulary(\n",
        "        self,\n",
        "        pad_token: str = \"<pad>\",\n",
        "        unk_token: str = \"<unk>\",\n",
        "        extra_tokens: list[str] = []\n",
        "    ) -> Vocab:\n",
        "        # most_common() returns a list of (token, count) pairs, so we convert them back into dictionary\n",
        "        # counts how many times a token appears in the dataset\n",
        "        vocab_counter = dict(Counter(token for sample in self.samples for token in sample[\"tokens\"]).most_common())\n",
        "\n",
        "        # We build the vocabulary\n",
        "        vocabulary = vocab(vocab_counter, min_freq=1, specials=[pad_token, unk_token, *extra_tokens])\n",
        "\n",
        "        # the default index is the ID used when you have a token not present in the vocab.\n",
        "        vocabulary.set_default_index(vocabulary([unk_token])[0])\n",
        "\n",
        "        return vocabulary\n",
        "\n",
        "    def set_padding_id(self, value: int) -> None:\n",
        "        self.padding_id = value\n",
        "\n",
        "    def index(self, vocab: dict[str, torch.Tensor]) -> None:\n",
        "        if self.indexed_data is not None:\n",
        "            print(\"Dataset has already been indexed. Keeping old index...\")\n",
        "        else:\n",
        "            indexed_data = []\n",
        "            for sample in self.samples:\n",
        "                # append the dictionary containing ids of the input tokens and label\n",
        "                indexed_data.append({\"input_ids\": vocabulary(sample[\"tokens\"]), \"label\": sample[\"label\"]})\n",
        "            self.indexed_data = indexed_data\n",
        "\n",
        "    def _collate_fn(self, raw_batch: list[dict]) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "\n",
        "        if self.padding_id is None:\n",
        "            raise RuntimeError(\"Padding value not set! Set it through .set_padding_id method.\")\n",
        "\n",
        "        # We need these sequence lengths to construct a `torch.nn.utils.rnn.PackedSequence` in the model\n",
        "        sequence_lengths = torch.tensor([len(sample[\"input_ids\"]) for sample in raw_batch], dtype=torch.long)\n",
        "        padded_sequence = pad_sequence(\n",
        "            (\n",
        "                torch.tensor(sample[\"input_ids\"], dtype=torch.long, device=self.device)\n",
        "                for sample in raw_batch\n",
        "            ),\n",
        "            batch_first=True,\n",
        "            padding_value=self.padding_id\n",
        "        )\n",
        "        labels = torch.tensor([sample[\"label\"] for sample in raw_batch], device=self.device, dtype=torch.long)\n",
        "        return sequence_lengths, padded_sequence, labels"
      ],
      "metadata": {
        "id": "jUsUeZ-YAvr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model"
      ],
      "metadata": {
        "id": "4vNAt9ZxPKwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTMModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocabulary_length: int,\n",
        "        hidden_dim: int,\n",
        "        bilstm_layers: int,\n",
        "        bilstm_dropout: float,\n",
        "        num_classes: int,\n",
        "        padding_id: int,\n",
        "        device: str = \"cuda\",\n",
        "        pre_embeddings: torch.Tensor = None,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = torch.device(device)\n",
        "\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocabulary_length,\n",
        "            embedding_dim=hidden_dim,\n",
        "            padding_idx=padding_id, # avoid updating the gradient of padding entries\n",
        "            device=self.device\n",
        "        )\n",
        "        if pre_embeddings is not None:\n",
        "            print(\"initializing embeddings from pretrained\")\n",
        "            self.embedding.weight.data.copy_(pre_embeddings)\n",
        "\n",
        "        self.bilstm = nn.LSTM(\n",
        "            input_size=hidden_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=bilstm_layers,\n",
        "            batch_first=True,\n",
        "            dropout=bilstm_dropout,\n",
        "            bidirectional=True,\n",
        "            device=self.device\n",
        "        )\n",
        "\n",
        "        self.projection = nn.Linear(\n",
        "            in_features=hidden_dim * 2,\n",
        "            out_features=num_classes,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "    def forward(self, batch: tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n",
        "        # Get the different parts of the batch\n",
        "        sequence_lengths, input_ids = batch\n",
        "\n",
        "        # First we embed the input tokens\n",
        "        embeds = self.embedding(input_ids) # [B, S, H]\n",
        "        # where B is the batch size, S is the sequence length and H is the hidden dimension\n",
        "\n",
        "        # Pack the sequence to avoid gradient descent on padding tokens.\n",
        "        # An alternative to packing sequences is using masking.\n",
        "        packed = pack_padded_sequence(embeds, sequence_lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # Then we pass it to the BiLSTM\n",
        "        # The first output of the BiLSTM tuple, packed_output, is of size B x S x 2H,\n",
        "        # where B is the batch size, S is the sequence length and H is the hidden dimension\n",
        "        # hidden_state is of size [2 * num_layers, B, H], where the 2 is because we are using BiLSTMs instead of LSTMs.\n",
        "        # cell_state has size [2 * num_layers, B, C] where C is the cell dimension of the internal LSTMCell.\n",
        "        packed_output, (hidden_state, cell_state) = self.bilstm(packed)\n",
        "\n",
        "        # We take the last two hidden representations of the BiLSTM (the second-to-last layer's output is forward; last\n",
        "        # layer's is backward) by concatenating forward and backward over dimension 1.\n",
        "        # Both tensors have shapes of [B, H], so concatenating them along the second dimension (dim 1) results in a new\n",
        "        # tensor of shape [B, 2 * H]\n",
        "        hidden = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1)\n",
        "\n",
        "        # Finally we project to the two final classes and return the logits of each class\n",
        "        # logits = values assigned to each class\n",
        "        logits = self.projection(hidden) # [B, 2]\n",
        "        return logits"
      ],
      "metadata": {
        "id": "yc1HWmWPPMfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainer\n"
      ],
      "metadata": {
        "id": "FVjVl-sZWGl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer():\n",
        "    \"\"\"Utility class to train and evaluate a model.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        log_steps: int = 1_000,\n",
        "        log_level: int = 2\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_function = nn.CrossEntropyLoss() # this is the default loss used nearly everywhere in NLP\n",
        "\n",
        "\n",
        "        self.scheduler = None\n",
        "\n",
        "        self.log_steps = log_steps\n",
        "        self.log_level = log_level\n",
        "\n",
        "    def set_scheduler(self, lr_decay_factor: float):\n",
        "        self.scheduler = lr_scheduler.StepLR(self.optimizer, step_size=1, gamma=lr_decay_factor)\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        train_dataloader: DataLoader,\n",
        "        valid_dataloader: DataLoader,\n",
        "        epochs: int = 1,\n",
        "        patience: int = 2,\n",
        "        lr_decay_factor: float = 0.001\n",
        "    ) -> dict[str, list[float]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            train_dataloader: a DataLoader instance containing the training instances.\n",
        "            valid_dataloader: a DataLoader instance used to evaluate learning progress.\n",
        "            epochs: the number of times to iterate over train_dataset.\n",
        "\n",
        "        Returns:\n",
        "            avg_train_loss: the average training loss on train_dataset over epochs.\n",
        "        \"\"\"\n",
        "        assert epochs >= 1 and isinstance(epochs, int)\n",
        "        if self.log_level > 0:\n",
        "            print('Training ...')\n",
        "        train_loss = 0.0\n",
        "\n",
        "        losses = {\n",
        "            \"train_losses\": [],\n",
        "            \"valid_losses\": [],\n",
        "            \"valid_acc\": [],\n",
        "        }\n",
        "\n",
        "        consecutive_increases = 0\n",
        "        prev_valid_loss = float('inf')\n",
        "\n",
        "        if self.scheduler is None:\n",
        "            self.set_scheduler(lr_decay_factor)\n",
        "\n",
        "        done = False\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            if self.log_level > 0:\n",
        "                print(' Epoch {:2d}'.format(epoch))\n",
        "\n",
        "            epoch_loss = 0.0\n",
        "            self.model.train()\n",
        "\n",
        "            # for each batch\n",
        "            for step, (sequence_lengths, inputs, labels) in enumerate(train_dataloader):\n",
        "                # we need to set the gradients to zero before starting to do backpropragation\n",
        "                # because PyTorch accumulates the gradients on subsequent backward passes\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # We get the predicted logits from the model, with no need to perform any flattening\n",
        "                # as both predictions and labels refer to the whole sentence.\n",
        "                predictions = self.model((sequence_lengths, inputs))\n",
        "\n",
        "                # The CrossEntropyLoss expects the predictions to be logits, i.e. non-softmaxed scores across\n",
        "                # the number of classes, and the labels to be a simple tensor of labels.\n",
        "                # Specifically, predictions needs to be of shape [B, C], where B is the batch size and C is the number of\n",
        "                # classes, while labels must be of shape [B] where each element l_i should 0 <= l_i < C.\n",
        "                # See https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html for more information.\n",
        "                sample_loss = self.loss_function(predictions, labels)\n",
        "                sample_loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                epoch_loss += sample_loss.cpu().tolist()\n",
        "\n",
        "                if self.log_level > 1 and (step % self.log_steps) == (self.log_steps - 1):\n",
        "                    print('\\t[E: {:2d} @ step {}] current avg loss = {:0.4f}'.format(epoch, step, epoch_loss / (step + 1)))\n",
        "\n",
        "            avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
        "\n",
        "            if self.log_level > 0:\n",
        "                print('\\t[E: {:2d}] train loss = {:0.4f}'.format(epoch, avg_epoch_loss))\n",
        "\n",
        "            valid_loss, valid_acc = self.evaluate(valid_dataloader)\n",
        "\n",
        "            losses[\"train_losses\"].append(avg_epoch_loss)\n",
        "            losses[\"valid_losses\"].append(valid_loss)\n",
        "            losses[\"valid_acc\"].append(valid_acc)\n",
        "\n",
        "            if self.log_level > 0:\n",
        "                print('  [E: {:2d}] valid loss = {:0.4f}, valid acc = {:0.4f}'.format(epoch, valid_loss, valid_acc))\n",
        "\n",
        "            if valid_loss < 0.485 and done == False:\n",
        "                done = True\n",
        "                for param_group in self.optimizer.param_groups:\n",
        "                    self.scheduler.step()\n",
        "                if self.log_level > 0:\n",
        "                    print('Reducing LR')\n",
        "\n",
        "            # *********** if the val loss is decreasing for patience-times -> early stop *********** #\n",
        "            if valid_loss > prev_valid_loss:\n",
        "                consecutive_increases += 1\n",
        "                if self.log_level > 0:\n",
        "                  print('   Consecutive increases: ' + str(consecutive_increases))\n",
        "            else:\n",
        "                consecutive_increases = 0\n",
        "\n",
        "            if consecutive_increases >= patience:\n",
        "                if self.log_level > 0:\n",
        "                    print('early stopping')\n",
        "                break\n",
        "\n",
        "            if consecutive_increases == 0:\n",
        "                prev_valid_loss = valid_loss\n",
        "            # *********** #\n",
        "\n",
        "        if self.log_level > 0:\n",
        "            print('... Done!')\n",
        "\n",
        "        return losses\n",
        "\n",
        "\n",
        "    def _compute_acc(self, logits: torch.Tensor, labels: torch.Tensor) -> float:\n",
        "        # logits [B, 2] are the logits outputted by the BiLSTM model's forward()\n",
        "        # We take the argmax along the second dimension (dim=1), so we get a tensor of shape [B]\n",
        "        # where each element is 0 if the 0-class had higher logit, 1 otherwise.\n",
        "        predictions = torch.argmax(logits, dim=1)\n",
        "        # We can then directly compare each prediction with the labels, as they are both tensors with shape [B].\n",
        "        # The average of the boolean equality checks between the two is the accuracy of these predictions.\n",
        "        # For example, if:\n",
        "        #   predictions = [1, 0, 0, 1, 1]\n",
        "        #   labels = [1, 0, 1, 1, 1]\n",
        "        # The comparison is:\n",
        "        #   (predictions == labels) => [1, 1, 0, 1, 1]\n",
        "        # which averaged gives an accuracy of 4/5, i.e. 0.80.\n",
        "        return torch.mean((predictions == labels).float()).tolist() # type: ignore\n",
        "\n",
        "    def evaluate(self, valid_dataloader: DataLoader) -> tuple[float, float]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            valid_dataloader: the DataLoader to use to evaluate the model.\n",
        "\n",
        "        Returns:\n",
        "            avg_valid_loss: the average validation loss over valid_dataloader.\n",
        "        \"\"\"\n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        # When running in inference mode, it is required to have model.eval() AND .no_grad()\n",
        "        # Among other things, these set dropout to 0 and turn off gradient computation.\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in valid_dataloader:\n",
        "                sequence_lengths, inputs, labels = batch\n",
        "\n",
        "                logits = self.model((sequence_lengths, inputs))\n",
        "\n",
        "                # Same considerations as the training step apply here\n",
        "                sample_loss = self.loss_function(logits, labels)\n",
        "                valid_loss += sample_loss.tolist()\n",
        "\n",
        "                sample_acc = self._compute_acc(logits, labels)\n",
        "                valid_acc += sample_acc\n",
        "\n",
        "        return valid_loss / len(valid_dataloader), valid_acc / len(valid_dataloader),\n",
        "\n",
        "    def predict(self, batch: tuple[torch.Tensor, torch.Tensor]) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: a tensor of indices\n",
        "        Returns:\n",
        "            A tuple composed of:\n",
        "            - the logits of each class, 0 and 1\n",
        "            - the prediction for each sample in the batch\n",
        "              0 if the sentiment of the sentence is negative, 1 if it is positive.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            sequence_lengths, inputs = batch\n",
        "            logits = self.model(sequence_lengths, inputs) # [B, 2]\n",
        "            predictions = torch.argmax(logits, -1) # [B, 1] computed on the last dimension of the logits tensor\n",
        "            return logits, predictions"
      ],
      "metadata": {
        "id": "Zy1m59Z3U7tS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "ecBM6lYuxLw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pad_token, unk_token = \"<pad>\", \"<unk>\"\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "XaxtKKmoh3p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "train_dataset = SST2Dataset(train_path, device=device)\n",
        "validation_dataset = SST2Dataset(validation_path, device=device)\n",
        "test_tweets_dataset = SST2Dataset(test_tweets_path, device=device)\n",
        "test_news_dataset = SST2Dataset(test_news_path, device=device)\n",
        "\n",
        "vocabulary = train_dataset.get_vocabulary(pad_token=pad_token, unk_token=unk_token)\n",
        "padding_id = vocabulary([pad_token])[0]\n",
        "\n",
        "train_dataset.set_padding_id(padding_id)\n",
        "validation_dataset.set_padding_id(padding_id)\n",
        "test_tweets_dataset.set_padding_id(padding_id)\n",
        "test_news_dataset.set_padding_id(padding_id)\n",
        "\n",
        "train_dataset.index(vocabulary)\n",
        "validation_dataset.index(vocabulary)\n",
        "test_tweets_dataset.index(vocabulary)\n",
        "test_news_dataset.index(vocabulary)\n",
        "\n",
        "print(f\"Training len: {len(train_dataset)}\")\n",
        "print(f\"Validation len: {len(validation_dataset)}\")\n",
        "print(f\"Test tweets len: {len(test_tweets_dataset)}\")\n",
        "print(f\"Test news len: {len(test_news_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q131m0izb_JO",
        "outputId": "f5b7bc5c-08ee-4e40-e32e-237536ec60e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training len: 5472\n",
            "Validation len: 1367\n",
            "Test tweets len: 1263\n",
            "Test news len: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "missing_file = \"missing_words.txt\"\n",
        "founded_file = \"founded_words.txt\"\n",
        "\n",
        "with open(missing_file, \"w\") as miss:\n",
        "  with open(founded_file, \"w\") as find:\n",
        "    pretrained_embeddings = torch.randn(len(vocabulary), 300)\n",
        "    initialised = 0\n",
        "    for i, w in enumerate(vocabulary.get_itos()):\n",
        "        word_low = w.lower()\n",
        "        if w in embedded_words:\n",
        "            initialised += 1\n",
        "            pretrained_embeddings[i] = torch.from_numpy(model.get_word_vector(w)).float()\n",
        "            find.write(w + \"\\n\")\n",
        "        elif word_low in embedded_words:\n",
        "            initialised += 1\n",
        "            pretrained_embeddings[i] = torch.from_numpy(model.get_word_vector(word_low)).float()\n",
        "            find.write(w + \" -> \" + word_low + \"\\n\")\n",
        "        else:\n",
        "            miss.write(w + \"\\n\")\n",
        "\n",
        "pretrained_embeddings[vocabulary[\"<pad>\"]] = torch.zeros(300)\n",
        "print(\"initialised embeddings {}\".format(initialised))\n",
        "print(\"random initialised embeddings {} \".format(len(vocabulary) - initialised))"
      ],
      "metadata": {
        "id": "W3sKjDLnMcyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "results = {}\n",
        "custom_epochs= 60\n",
        "custom_hidden_dim = 300\n",
        "custom_patience = 3\n",
        "custom_batch_size = 32\n",
        "custom_bilstm_layers = 3\n",
        "custom_bilstm_dropout = 0.2\n",
        "custom_lr = 5e-5\n",
        "\n",
        "training_dataloader = DataLoader(train_dataset, batch_size=custom_batch_size, shuffle=True, collate_fn=train_dataset._collate_fn)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=custom_batch_size, shuffle=False, collate_fn=validation_dataset._collate_fn)\n",
        "test_news_dataloader = DataLoader(test_news_dataset, batch_size=custom_batch_size, shuffle=False, collate_fn=test_news_dataset._collate_fn)\n",
        "test_tweets_dataloader = DataLoader(test_tweets_dataset, batch_size=custom_batch_size, shuffle=False, collate_fn=test_tweets_dataset._collate_fn)\n",
        "\n",
        "while(True):\n",
        "\n",
        "  sentiment_tagger = BiLSTMModel(\n",
        "      vocabulary_length=len(vocabulary),\n",
        "      hidden_dim=custom_hidden_dim,\n",
        "      bilstm_layers=custom_bilstm_layers,\n",
        "      bilstm_dropout=custom_bilstm_dropout,\n",
        "      num_classes=2,\n",
        "      padding_id=padding_id,\n",
        "      device=device,\n",
        "      pre_embeddings=pretrained_embeddings\n",
        "  )\n",
        "\n",
        "  trainer = Trainer(\n",
        "      model=sentiment_tagger,\n",
        "      optimizer=torch.optim.Adam(sentiment_tagger.parameters(), lr=custom_lr),\n",
        "      log_steps=100,\n",
        "      log_level=1\n",
        "  )\n",
        "\n",
        "  losses = trainer.train(training_dataloader, validation_dataloader, epochs=custom_epochs, patience=custom_patience)\n",
        "  test_news_loss, test_news_acc = trainer.evaluate(test_news_dataloader)\n",
        "  test_tweets_loss, test_tweets_acc = trainer.evaluate(test_tweets_dataloader)\n",
        "  params = (custom_batch_size, custom_hidden_dim, custom_bilstm_layers, custom_bilstm_dropout, custom_lr)\n",
        "  results[params] = {\n",
        "      'train_losses': losses['train_losses'],\n",
        "      'valid_losses': losses['valid_losses'],\n",
        "      'valid_acc': losses['valid_acc'],\n",
        "      'test_news_loss': test_news_loss,\n",
        "      'test_news_acc': test_news_acc,\n",
        "      'test_tweets_loss': test_tweets_loss,\n",
        "      'test_tweets_acc': test_tweets_acc\n",
        "  }\n",
        "\n",
        "  print(params)\n",
        "  print(\"valid_acc: \" + str(losses['valid_acc'][len(losses['valid_acc']) - 1]))\n",
        "  print(\"loss: \" + str(losses['valid_losses'][len(losses['valid_losses']) - 1]))\n",
        "  print(\"test_news_acc: \" + str(test_news_acc))\n",
        "  print(\"test_tweets_acc: \" + str(test_tweets_acc))\n",
        "\n",
        "  print()\n",
        "\n",
        "  check = losses['valid_losses'][len(losses['valid_losses']) - 1]\n",
        "  if check < 0.48:\n",
        "    break"
      ],
      "metadata": {
        "id": "wGs1zCkuhvc_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2508ae09-495a-4ecf-beee-179a5dcb3f12"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initializing embeddings from pretrained\n",
            "Training ...\n",
            " Epoch  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = results[params]['train_losses']\n",
        "valid_losses = results[params]['valid_losses']\n",
        "valid_acc = results[params]['valid_acc']\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(valid_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZMqazglY88iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(valid_acc, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OeRi_Y8R9mG8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}